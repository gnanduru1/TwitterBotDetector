{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ef6a207-ce51-47bd-8737-e29c9935ab11",
   "metadata": {},
   "source": [
    "# Experiments\n",
    "\n",
    "This notebook trains and evaluates several different models on the graph generated from the create_graph notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfaa6086-4e9f-4714-ba5c-915d2793d893",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:72: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/bae9wk/.local/lib/python3.10/site-packages/torch_scatter/_scatter_cuda.so: undefined symbol: _ZN3c104cuda14MaybeSetDeviceEa\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/usr/local/lib/python3.10/dist-packages/torch_geometric/typing.py:110: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/bae9wk/.local/lib/python3.10/site-packages/torch_sparse/_convert_cuda.so: undefined symbol: _ZN3c1013ParallelGuardC1Eb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "import getpass\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import WeightedRandomSampler\n",
    "from torch_geometric.nn.models import GAT\n",
    "from torch_geometric.nn import GCNConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841daede-f8f6-4732-b344-2700ee054360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefeee0b-fe73-4831-8871-2630834737bc",
   "metadata": {},
   "source": [
    "## WandB\n",
    "This project uses weights and biases (wandb) to store its training run data. Please install wandb and run `wandb login` to enable wandb logging of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f032a042-9803-4e10-915b-4cb668c3b014",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user = getpass.getuser()\n",
    "# Change dirpath to the location of the TwiBot22 dataset on your device\n",
    "dirpath = '/scratch/{user}/datasets/TwiBot22/'\n",
    "\n",
    "user = pd.read_csv(f'{dirpath}/user_final.csv')\n",
    "graph = pd.read_csv(f'{dirpath}/graph_cleaned.csv', float_precision = 'round_trip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a0c94c9-7b0a-4168-a20b-1d44a91e2aa4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Function to standardize target_user_id format\n",
    "def standardize_target_id(target_id):\n",
    "    try:\n",
    "        # If it's already a string representation of a list\n",
    "        if isinstance(target_id, str):\n",
    "            if target_id.startswith('['):\n",
    "                return ast.literal_eval(target_id)\n",
    "            else:\n",
    "                return [int(float(target_id))]\n",
    "        # If it's a float\n",
    "        elif isinstance(target_id, float):\n",
    "            if pd.isna(target_id):\n",
    "                return []\n",
    "            return [int(target_id)]\n",
    "        # If it's already a list\n",
    "        elif isinstance(target_id, list):\n",
    "            return target_id\n",
    "        # If it's an integer\n",
    "        elif isinstance(target_id, (int, np.integer)):\n",
    "            return [target_id]\n",
    "        else:\n",
    "            print(type(target_id), target_id)\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        print(f\"Exception on target id {target_id}: {e}\")\n",
    "        return []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b37acc0-ea9d-4bdc-a967-c8a141fb856b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length: 615613\n",
      "Length after cleaning: 510190\n",
      "\n",
      "Sample of cleaned target_user_id types:\n",
      "0     <class 'numpy.ndarray'>\n",
      "1     <class 'numpy.ndarray'>\n",
      "12    <class 'numpy.ndarray'>\n",
      "13    <class 'numpy.ndarray'>\n",
      "14    <class 'numpy.ndarray'>\n",
      "Name: target_user_id, dtype: object\n",
      "\n",
      "Sample of cleaned target_user_id values:\n",
      "0              [3123238004]\n",
      "1               [343627165]\n",
      "12    [1056221232822022144]\n",
      "13     [993279617938042880]\n",
      "14               [15245653]\n",
      "Name: target_user_id, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Clean the graph DataFrame\n",
    "print(\"Original length:\", len(graph))\n",
    "\n",
    "# Standardize target_user_id format\n",
    "graph['target_user_id'] = graph['target_user_id'].apply(standardize_target_id)\n",
    "graph['target_user_id'] = graph['target_user_id'].apply(lambda x: np.array(x, dtype=np.int64))\n",
    "\n",
    "# Remove rows where target_user_id is empty\n",
    "graph = graph[graph['target_user_id'].apply(len) > 0]\n",
    "\n",
    "# Remove rows where source_user_id is not in valid_user_ids\n",
    "valid_user_ids = set(user['source_user_id'])\n",
    "graph = graph[graph['source_user_id'].isin(valid_user_ids)]\n",
    "\n",
    "print(\"Length after cleaning:\", len(graph))\n",
    "\n",
    "# Verify the cleaning worked\n",
    "print(\"\\nSample of cleaned target_user_id types:\")\n",
    "print(graph['target_user_id'].head().apply(type))\n",
    "print(\"\\nSample of cleaned target_user_id values:\")\n",
    "print(graph['target_user_id'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a647057-8d7c-4f32-99ab-b3dc325b7d44",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "510190it [00:22, 22270.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Now proceed with the original code\n",
    "user_to_index = {user_id: idx for idx, user_id in enumerate(user['source_user_id'])}\n",
    "labels = torch.tensor(user['label'].map({'human': 0, 'bot': 1}).values, dtype=torch.long)\n",
    "\n",
    "# Rest of the code remains the same...\n",
    "# Create edge list properly\n",
    "edge_list = []\n",
    "edge_weights = {}\n",
    "\n",
    "for _, row in tqdm(graph.iterrows()):\n",
    "    source_id = row['source_user_id']\n",
    "    target_ids = row['target_user_id']\n",
    "    #target_ids = ast.literal_eval(row['target_user_id']) if isinstance(row['target_user_id'], str) else [row['target_user_id']]\n",
    "    \n",
    "    source_idx = user_to_index[source_id]\n",
    "    for target_id in target_ids:\n",
    "        #print(target_id, target_ids)\n",
    "        if target_id in user_to_index:  # Check if target exists in our user set\n",
    "            target_idx = user_to_index[target_id]\n",
    "            pair = (source_idx, target_idx)\n",
    "            if pair not in edge_weights:\n",
    "                edge_list.append(pair)\n",
    "                edge_weights[pair] = 1\n",
    "            else:\n",
    "                edge_weights[pair] += 1\n",
    "\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "edge_weight = torch.tensor(list(edge_weights.values()), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22ba635b-28a6-443d-a5a9-e060916f3c1a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 189091/189091 [01:33<00:00, 2021.41it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create node features using tweet embeddings\n",
    "node_features = []\n",
    "for user_id in tqdm(user['source_user_id']):\n",
    "    user_tweets = graph[graph['source_user_id'] == user_id]\n",
    "    if len(user_tweets) > 0:\n",
    "        # Filter out NaN values and safely evaluate the embeddings\n",
    "        valid_embeddings = []\n",
    "        for emb in user_tweets['tweet_embedding'].values:\n",
    "            try:\n",
    "                if pd.notna(emb):  # Check if embedding is not NaN\n",
    "                    emb_str = emb.replace('[', '').replace(']', '')\n",
    "                    emb_values = [float(x) for x in emb_str.split()]\n",
    "                    valid_embeddings.append(emb_values)\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                continue\n",
    "        \n",
    "        if valid_embeddings:  # If we have any valid embeddings\n",
    "            embeddings = np.vstack(valid_embeddings)\n",
    "            node_features.append(torch.tensor(embeddings.mean(axis=0)))\n",
    "        else:\n",
    "            node_features.append(torch.zeros(100))  # Default for users with no valid embeddings\n",
    "    else:\n",
    "        node_features.append(torch.zeros(100))  # Default for users with no tweets\n",
    "\n",
    "node_features = torch.stack(node_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ff4ab7-5545-4b7a-b766-5e8bf02209b4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of zero tensors: 70446 out of 189091 (37.26%)\n",
      "\n",
      "First few user IDs with zero embeddings:\n",
      "3     138814032\n",
      "4     457554412\n",
      "5    2465283662\n",
      "6     284870222\n",
      "9      83389771\n",
      "Name: source_user_id, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Count zero tensors\n",
    "zero_tensor = torch.zeros(100)  # Assuming 100-dimensional embeddings\n",
    "zero_count = sum((tensor == zero_tensor).all().item() for tensor in node_features)\n",
    "total_count = len(node_features)\n",
    "\n",
    "print(f\"Number of zero tensors: {zero_count} out of {total_count} ({(zero_count/total_count)*100:.2f}%)\")\n",
    "\n",
    "# To see which user IDs have zero embeddings:\n",
    "zero_indices = [i for i, tensor in enumerate(node_features) if (tensor == zero_tensor).all().item()]\n",
    "zero_user_ids = user['source_user_id'].iloc[zero_indices]\n",
    "\n",
    "print(\"\\nFirst few user IDs with zero embeddings:\")\n",
    "print(zero_user_ids.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bd98084b-5d3c-4166-af3a-633a7f5a632e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of zero tensors: 0 out of 189091 (0.00%)\n",
      "Feature dimension: 100\n",
      "\n",
      "Sample non-zero features (first 10 values):\n",
      "tensor([-0.5917, -0.1012,  0.2680,  0.1024, -0.0170,  0.2148, -0.1373, -0.5112,\n",
      "         0.1039, -0.4348], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Feature infilling if text embedding = 0\n",
    "\n",
    "# Select numerical columns from user DataFrame\n",
    "numerical_features = user.select_dtypes(include=['float64', 'int64']).columns\n",
    "user_numerical = user[numerical_features]\n",
    "\n",
    "# Manual normalization function\n",
    "def normalize_features(df):\n",
    "    result = df.copy()\n",
    "    for column in df.columns:\n",
    "        mean = df[column].mean()\n",
    "        std = df[column].std()\n",
    "        if std != 0:\n",
    "            result[column] = (df[column] - mean) / std\n",
    "        else:\n",
    "            result[column] = 0  # For constant columns\n",
    "    return result\n",
    "\n",
    "# Normalize the features\n",
    "user_features = normalize_features(user_numerical)\n",
    "user_features = torch.tensor(user_features.values, dtype=torch.float)\n",
    "\n",
    "# Combine with tweet embeddings\n",
    "combined_features = []\n",
    "zero_tensor = torch.zeros(node_features.size(1))\n",
    "\n",
    "for i, user_id in enumerate(user['source_user_id']):\n",
    "    if (node_features[i] == zero_tensor).all():\n",
    "        # If no tweet embeddings, use processed user features\n",
    "        # Pad or truncate user features to match embedding dimension if necessary\n",
    "        if user_features.size(1) > 100:\n",
    "            combined_features.append(user_features[i][:100])\n",
    "        elif user_features.size(1) < 100:\n",
    "            padded = torch.zeros(100)\n",
    "            padded[:user_features.size(1)] = user_features[i]\n",
    "            combined_features.append(padded)\n",
    "        else:\n",
    "            combined_features.append(user_features[i])\n",
    "    else:\n",
    "        # If we have tweet embeddings, use those\n",
    "        combined_features.append(node_features[i])\n",
    "\n",
    "combined_features = torch.stack(combined_features)\n",
    "\n",
    "# Verify results\n",
    "zero_tensor = torch.zeros(combined_features.size(1))\n",
    "zero_count = sum((tensor == zero_tensor).all().item() for tensor in combined_features)\n",
    "total_count = len(combined_features)\n",
    "\n",
    "print(f\"\\nNumber of zero tensors: {zero_count} out of {total_count} ({(zero_count/total_count)*100:.2f}%)\")\n",
    "print(f\"Feature dimension: {combined_features.size(1)}\")\n",
    "\n",
    "# Print a sample of non-zero features to verify they look correct\n",
    "non_zero_idx = (combined_features != zero_tensor).any(dim=1).nonzero().squeeze()\n",
    "if len(non_zero_idx) > 0:\n",
    "    print(\"\\nSample non-zero features (first 10 values):\")\n",
    "    print(combined_features[non_zero_idx[0]][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "686b5c15-44d3-4d55-a6f8-9c7cc0b98943",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GATModel(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2, heads=1, dropout=0.6):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.gat = GAT(\n",
    "            in_channels=in_channels,\n",
    "            hidden_channels=hidden_channels,  \n",
    "            out_channels=out_channels, \n",
    "            num_layers=num_layers,\n",
    "            heads=heads,\n",
    "            dropout=dropout,\n",
    "            act=F.elu,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        return self.gat(x, edge_index, edge_weight=edge_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9097c2f3-b821-47a1-8d81-731ffe11002e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Example split (80% train, 10% val, 10% test)\n",
    "train_mask, test_mask = train_test_split(torch.arange(labels.size(0)), test_size=0.2, stratify=labels)\n",
    "val_mask, test_mask = train_test_split(test_mask, test_size=0.5, stratify=labels[test_mask])\n",
    "\n",
    "# Convert to boolean masks\n",
    "train_mask = torch.zeros_like(labels, dtype=torch.bool).scatter_(0, train_mask, True)\n",
    "val_mask = torch.zeros_like(labels, dtype=torch.bool).scatter_(0, val_mask, True)\n",
    "test_mask = torch.zeros_like(labels, dtype=torch.bool).scatter_(0, test_mask, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f514a251-e540-409b-b4c3-60b196b4c5a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "data, labels, edge_index, edge_weight = combined_features.to(device), labels.to(device), edge_index.to(device), edge_weight.to(device)\n",
    "data = data.float()  # Convert node features to float32\n",
    "edge_index = edge_index.long()  # Edge indices should be long\n",
    "labels = labels.long() \n",
    "\n",
    "data = (data - data.mean(dim=0)) / data.std(dim=0)\n",
    "data = torch.nan_to_num(data, nan=0.0)  # Replace NaN with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8626f749-9bf8-480d-86fa-44f66b8464eb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human tweets: 156139, Bot tweets: 16476\n",
      "Bot percentage: 8.713264465332031\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Label balance\n",
    "\n",
    "num_bots = torch.sum(labels == 1)\n",
    "num_humans = torch.sum(labels == 0)\n",
    "print(f\"\"\"Human tweets: {num_humans-num_bots}, Bot tweets: {num_bots}\n",
    "Bot percentage: {100*num_bots / len(labels)}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df3f15ed-6fcc-4384-9a5f-9eb9dcd0f8c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189091"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9cfb0-52dd-4b74-a967-b9eb4257fa77",
   "metadata": {},
   "source": [
    "### Only need to run 1 of the following two cells containing GAT instantiation and training loop. The difference is that one set logs to wandb and the other doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "33917535-1474-4d1f-a304-7d092639dda4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = GATModel(\n",
    "    in_channels=data.size(1),\n",
    "    hidden_channels=64,\n",
    "    out_channels=2,\n",
    "    num_layers=2,\n",
    "    heads=4,\n",
    "    dropout=0.6\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data, edge_index, edge_weight=edge_weight)\n",
    "    \n",
    "    # Compute loss on training nodes\n",
    "    loss = criterion(out[train_mask], labels[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = criterion(out[val_mask], labels[val_mask])\n",
    "        val_acc = (out[val_mask].argmax(dim=1) == labels[val_mask]).float().mean()\n",
    "    \n",
    "    #if epoch % 20 == 0:\n",
    "    #print(f'Epoch: {epoch+1}, Loss: {loss.item():.4f}, Val Loss: {val_loss.item():.4f}, Val Acc: {val_acc:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "10025356-336f-4950-92b0-1e63c3afa3d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbae9wk\u001b[0m (\u001b[33mcrg\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/sfs/weka/scratch/bae9wk/TwiBot-22/wandb/run-20241130_142327-udbxbt51</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/crg/gat-classification/runs/udbxbt51' target=\"_blank\">pretty-disco-8</a></strong> to <a href='https://wandb.ai/crg/gat-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/crg/gat-classification' target=\"_blank\">https://wandb.ai/crg/gat-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/crg/gat-classification/runs/udbxbt51' target=\"_blank\">https://wandb.ai/crg/gat-classification/runs/udbxbt51</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▇▇███</td></tr><tr><td>train_acc</td><td>▁▃▅▅▆▆▇▇▇▇██████████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▇▆▅▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▃▃▅▅▆▇▇▇▇▇▇████████████████████████████</td></tr><tr><td>val_loss</td><td>██▇▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>999</td></tr><tr><td>train_acc</td><td>0.91382</td></tr><tr><td>train_loss</td><td>0.29966</td></tr><tr><td>val_acc</td><td>0.91359</td></tr><tr><td>val_loss</td><td>0.29964</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">pretty-disco-8</strong> at: <a href='https://wandb.ai/crg/gat-classification/runs/udbxbt51' target=\"_blank\">https://wandb.ai/crg/gat-classification/runs/udbxbt51</a><br/> View project at: <a href='https://wandb.ai/crg/gat-classification' target=\"_blank\">https://wandb.ai/crg/gat-classification</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20241130_142327-udbxbt51/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr = 1e-3\n",
    "epochs = 1000\n",
    "heads = 4\n",
    "dropout = 0.6\n",
    "weight_decay = 5e-4\n",
    "\n",
    "# Initialize wandb\n",
    "wandb.init(\n",
    "    project=\"gat-classification\",  # Choose your project name\n",
    "    config={\n",
    "        \"architecture\": \"GAT\",\n",
    "        \"in_channels\": data.size(1),\n",
    "        \"hidden_channels\": 64,\n",
    "        \"out_channels\": 2,\n",
    "        \"num_layers\": 2,\n",
    "        \"heads\": heads,\n",
    "        \"dropout\": dropout,\n",
    "        \"learning_rate\": lr,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"epochs\": epochs\n",
    "    }\n",
    ")\n",
    "\n",
    "model = GATModel(\n",
    "    in_channels=data.size(1),\n",
    "    hidden_channels=64,\n",
    "    out_channels=2,\n",
    "    num_layers=2,\n",
    "    heads=heads,\n",
    "    dropout=dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(data, edge_index, edge_weight=edge_weight)\n",
    "    \n",
    "    # Compute training metrics\n",
    "    train_loss = criterion(out[train_mask], labels[train_mask])\n",
    "    train_acc = (out[train_mask].argmax(dim=1) == labels[train_mask]).float().mean()\n",
    "    \n",
    "    # Backward pass\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss = criterion(out[val_mask], labels[val_mask])\n",
    "        val_acc = (out[val_mask].argmax(dim=1) == labels[val_mask]).float().mean()\n",
    "    \n",
    "    # Log metrics to wandb\n",
    "    wandb.log({\n",
    "        \"epoch\": epoch,\n",
    "        \"train_loss\": train_loss.item(),\n",
    "        \"train_acc\": train_acc.item(),\n",
    "        \"val_loss\": val_loss.item(),\n",
    "        \"val_acc\": val_acc.item()\n",
    "    })\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a25734dc-94ee-4935-9e28-c031412e89bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189091])\n",
      "torch.Size([18910])\n",
      "Test Accuracy: 91.37%\n",
      "R² Score: 0.0622\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our GAT model\n",
    "model.eval()\n",
    "\n",
    "# Forward pass on the test set\n",
    "with torch.no_grad():\n",
    "    out = model(data, edge_index)\n",
    "    \n",
    "    # Get predicted class by taking the argmax across the output logits\n",
    "    _, predicted = out.max(dim=1)\n",
    "    print(predicted.shape)\n",
    "    print(labels[test_mask].shape)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = (predicted[test_mask] == labels[test_mask]).sum().item()\n",
    "    total = test_mask.sum().item()\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Calculate R2\n",
    "    predicted_probs = torch.softmax(out[test_mask], dim=1)[:, 1].cpu().numpy()\n",
    "    true_values = labels[test_mask].cpu().numpy()\n",
    "    r2 = r2_score(true_values, predicted_probs)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "40fac691-e02d-4b05-96b3-bf72bc4af922",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[17231    31]\n",
      " [ 1600    48]]\n"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(labels[test_mask].cpu().numpy(), predicted[test_mask].cpu().numpy())\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c9e2eb0-0866-4ab3-8e90-4d0881be75ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GCNCluster(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels=64):\n",
    "        super(GCNCluster, self).__init__()\n",
    "        # First Graph Convolution Layer\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        # Second Graph Convolution Layer\n",
    "        self.conv2 = GCNConv(hidden_channels, 2)  # 2 classes (binary)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        # First layer with ReLU activation\n",
    "        x = self.conv1(x, edge_index, edge_weight)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        \n",
    "        # Second layer\n",
    "        x = self.conv2(x, edge_index, edge_weight)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "# Training setup\n",
    "def train_model(data, labels, edge_index, edge_weight, num_epochs=200):\n",
    "    model = GCNCluster(num_features=data.shape[1]).cuda()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data, edge_index, edge_weight)\n",
    "        loss = F.nll_loss(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            pred = out.argmax(dim=1)\n",
    "            correct = (pred == labels).sum()\n",
    "            acc = int(correct) / len(labels)\n",
    "            print(f'Epoch {epoch:3d}, Loss: {loss:.4f}, Accuracy: {acc:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6b40de57-92f4-4817-8e23-59421fcfdde7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Loss: 1.6052, Accuracy: 0.3551\n",
      "Epoch  10, Loss: 0.5045, Accuracy: 0.9013\n",
      "Epoch  20, Loss: 0.4068, Accuracy: 0.8968\n",
      "Epoch  30, Loss: 0.3515, Accuracy: 0.9060\n",
      "Epoch  40, Loss: 0.3277, Accuracy: 0.9102\n",
      "Epoch  50, Loss: 0.3139, Accuracy: 0.9130\n",
      "Epoch  60, Loss: 0.3067, Accuracy: 0.9148\n",
      "Epoch  70, Loss: 0.2998, Accuracy: 0.9157\n",
      "Epoch  80, Loss: 0.2951, Accuracy: 0.9162\n",
      "Epoch  90, Loss: 0.2908, Accuracy: 0.9165\n",
      "Epoch 100, Loss: 0.2883, Accuracy: 0.9170\n",
      "Epoch 110, Loss: 0.2847, Accuracy: 0.9173\n",
      "Epoch 120, Loss: 0.2827, Accuracy: 0.9175\n",
      "Epoch 130, Loss: 0.2801, Accuracy: 0.9180\n",
      "Epoch 140, Loss: 0.2781, Accuracy: 0.9184\n",
      "Epoch 150, Loss: 0.2769, Accuracy: 0.9184\n",
      "Epoch 160, Loss: 0.2744, Accuracy: 0.9186\n",
      "Epoch 170, Loss: 0.2737, Accuracy: 0.9188\n",
      "Epoch 180, Loss: 0.2725, Accuracy: 0.9190\n",
      "Epoch 190, Loss: 0.2711, Accuracy: 0.9193\n"
     ]
    }
   ],
   "source": [
    "gcn_model = train_model(data, labels, edge_index, edge_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "454a9b6b-ec32-4b39-bedd-4fd34c638fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([189091])\n",
      "torch.Size([18910])\n",
      "Test Accuracy: 92.15%\n",
      "R² Score: 0.1390\n"
     ]
    }
   ],
   "source": [
    "# Evaluate our GCN model\n",
    "gcn_model.eval()\n",
    "\n",
    "# Forward pass on the test set\n",
    "with torch.no_grad():\n",
    "    out = gcn_model(data, edge_index) \n",
    "    \n",
    "    # Get predicted class by taking the argmax across the output logits\n",
    "    _, predicted = out.max(dim=1)\n",
    "    print(predicted.shape)\n",
    "    print(labels[test_mask].shape)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    correct = (predicted[test_mask] == labels[test_mask]).sum().item()\n",
    "    total = test_mask.sum().item()\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # Calculate R2\n",
    "    predicted_probs = torch.softmax(out[test_mask], dim=1)[:, 1].cpu().numpy()\n",
    "    true_values = labels[test_mask].cpu().numpy()\n",
    "    r2 = r2_score(true_values, predicted_probs)\n",
    "    \n",
    "    print(f\"Test Accuracy: {accuracy * 100:.2f}%\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ef5c5168-5f94-4327-9396-23aa2fc522d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# k means implementation\n",
    "class ClusteringModel:\n",
    "    def __init__(self, n_clusters=2):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def fit(self, data, labels):\n",
    "        # Scale data and generate k means cluster map\n",
    "        data_cpu = data.cpu().numpy()\n",
    "        labels_cpu = labels.cpu().numpy()\n",
    "        \n",
    "        data_scaled = self.scaler.fit_transform(data_cpu)\n",
    "        self.kmeans.fit(data_scaled)\n",
    "        cluster_labels = self.kmeans.labels_\n",
    "        \n",
    "        self.cluster_map = {}\n",
    "        for cluster in range(self.n_clusters):\n",
    "            mask = (cluster_labels == cluster)\n",
    "            if mask.any():\n",
    "                true_labels = labels_cpu[mask]\n",
    "                most_common = np.bincount(true_labels).argmax()\n",
    "                self.cluster_map[cluster] = most_common\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict(self, data):\n",
    "        # Converting from tensor to numpy so that the data is compatible\n",
    "        #    with sklearn functions\n",
    "        data_cpu = data.cpu().numpy()\n",
    "        data_scaled = self.scaler.transform(data_cpu)\n",
    "        \n",
    "        cluster_labels = self.kmeans.predict(data_scaled)\n",
    "        predictions = np.array([self.cluster_map[label] for label in cluster_labels])\n",
    "        return torch.tensor(predictions, device=data.device)\n",
    "\n",
    "def train_cluster_model(data, labels):\n",
    "    model = ClusteringModel(n_clusters=2)\n",
    "    model.fit(data, labels)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    predictions = model.predict(data)\n",
    "    acc = (predictions == labels).float().mean()\n",
    "    print(f'Clustering Accuracy: {acc:.4f}')\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7c0f3a7c-4da4-46f7-ad27-62c638944895",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bae9wk/.local/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustering Accuracy: 0.9129\n"
     ]
    }
   ],
   "source": [
    "kmeans_model = train_cluster_model(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b4d9fc9b-c5c6-4744-b4e2-3eddb7b0eed9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a balanced cluster net, we implemented this to be a baseline that weighted \n",
    "#    bot and human tweets more equally during training to address the class imbalance\n",
    "class BalancedClusterNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=64):\n",
    "        super(BalancedClusterNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.layer2 = nn.Linear(hidden_dim, hidden_dim // 2)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim // 2)\n",
    "        self.layer3 = nn.Linear(hidden_dim // 2, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.bn1(F.relu(self.layer1(x)))\n",
    "        x = F.dropout(x, p=0.3, training=self.training)\n",
    "        x = self.bn2(F.relu(self.layer2(x)))\n",
    "        x = self.layer3(x)\n",
    "        return x.squeeze(-1)  # Ensure output is [batch_size]\n",
    "\n",
    "def train_balanced_model(data, labels, num_epochs=200):\n",
    "    device = data.device\n",
    "    model = BalancedClusterNet(input_dim=data.shape[1]).to(device)\n",
    "    \n",
    "    # Compute class weights\n",
    "    num_samples = len(labels)\n",
    "    num_class_0 = (labels == 0).sum().item()\n",
    "    num_class_1 = (labels == 1).sum().item()\n",
    "    \n",
    "    # Create weighted loss function\n",
    "    pos_weight = torch.tensor([num_class_0/num_class_1]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "    \n",
    "    # Create weighted sampler\n",
    "    sample_weights = torch.zeros_like(labels, dtype=torch.float32)\n",
    "    sample_weights[labels == 0] = 1.0 / num_class_0\n",
    "    sample_weights[labels == 1] = 1.0 / num_class_1\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=num_samples, replacement=True)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                pred = (torch.sigmoid(outputs) > 0.5).long()\n",
    "                \n",
    "                # Calculate metrics for both classes\n",
    "                acc_class0 = ((pred == labels) & (labels == 0)).float().sum() / (labels == 0).float().sum()\n",
    "                acc_class1 = ((pred == labels) & (labels == 1)).float().sum() / (labels == 1).float().sum()\n",
    "                \n",
    "                # Calculate F1 score\n",
    "                tp = ((pred == 1) & (labels == 1)).float().sum()\n",
    "                fp = ((pred == 1) & (labels == 0)).float().sum()\n",
    "                fn = ((pred == 0) & (labels == 1)).float().sum()\n",
    "                precision = tp / (tp + fp + 1e-10)\n",
    "                recall = tp / (tp + fn + 1e-10)\n",
    "                f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "                \n",
    "                print(f'Epoch {epoch:3d}, Loss: {loss:.4f}')\n",
    "                print(f'Class 0 Accuracy: {acc_class0:.4f}, Class 1 Accuracy: {acc_class1:.4f}')\n",
    "                print(f'F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\\n')\n",
    "            model.train()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, data, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)\n",
    "        pred = (torch.sigmoid(outputs) > 0.5).long()\n",
    "        \n",
    "        # Calculate confusion matrix\n",
    "        tp = ((pred == 1) & (labels == 1)).float().sum()\n",
    "        tn = ((pred == 0) & (labels == 0)).float().sum()\n",
    "        fp = ((pred == 1) & (labels == 0)).float().sum()\n",
    "        fn = ((pred == 0) & (labels == 1)).float().sum()\n",
    "        \n",
    "        # Calculate metrics\n",
    "        accuracy = (tp + tn) / len(labels)\n",
    "        precision = tp / (tp + fp + 1e-10)\n",
    "        recall = tp / (tp + fn + 1e-10)\n",
    "        f1 = 2 * (precision * recall) / (precision + recall + 1e-10)\n",
    "        \n",
    "        print(\"\\nFinal Evaluation:\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Precision: {precision:.4f}\")\n",
    "        print(f\"Recall: {recall:.4f}\")\n",
    "        print(f\"F1 Score: {f1:.4f}\")\n",
    "        print(\"\\nConfusion Matrix:\")\n",
    "        print(f\"TN: {tn:.0f}, FP: {fp:.0f}\")\n",
    "        print(f\"FN: {fn:.0f}, TP: {tp:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "35f75a09-5450-4874-9708-f85a754f19f8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0, Loss: 1.3171\n",
      "Class 0 Accuracy: 0.5834, Class 1 Accuracy: 0.4361\n",
      "F1 Score: 0.1504, Precision: 0.0909, Recall: 0.4361\n",
      "\n",
      "Epoch  10, Loss: 1.0653\n",
      "Class 0 Accuracy: 0.5584, Class 1 Accuracy: 0.7808\n",
      "F1 Score: 0.2437, Precision: 0.1444, Recall: 0.7808\n",
      "\n",
      "Epoch  20, Loss: 0.9441\n",
      "Class 0 Accuracy: 0.6784, Class 1 Accuracy: 0.7286\n",
      "F1 Score: 0.2858, Precision: 0.1778, Recall: 0.7286\n",
      "\n",
      "Epoch  30, Loss: 0.8801\n",
      "Class 0 Accuracy: 0.7011, Class 1 Accuracy: 0.7353\n",
      "F1 Score: 0.3022, Precision: 0.1902, Recall: 0.7353\n",
      "\n",
      "Epoch  40, Loss: 0.8492\n",
      "Class 0 Accuracy: 0.6888, Class 1 Accuracy: 0.7629\n",
      "F1 Score: 0.3038, Precision: 0.1896, Recall: 0.7629\n",
      "\n",
      "Epoch  50, Loss: 0.8339\n",
      "Class 0 Accuracy: 0.7677, Class 1 Accuracy: 0.6925\n",
      "F1 Score: 0.3357, Precision: 0.2215, Recall: 0.6925\n",
      "\n",
      "Epoch  60, Loss: 0.8204\n",
      "Class 0 Accuracy: 0.7203, Class 1 Accuracy: 0.7530\n",
      "F1 Score: 0.3216, Precision: 0.2044, Recall: 0.7530\n",
      "\n",
      "Epoch  70, Loss: 0.8127\n",
      "Class 0 Accuracy: 0.7622, Class 1 Accuracy: 0.7187\n",
      "F1 Score: 0.3414, Precision: 0.2239, Recall: 0.7187\n",
      "\n",
      "Epoch  80, Loss: 0.8090\n",
      "Class 0 Accuracy: 0.7403, Class 1 Accuracy: 0.7456\n",
      "F1 Score: 0.3338, Precision: 0.2151, Recall: 0.7456\n",
      "\n",
      "Epoch  90, Loss: 0.8028\n",
      "Class 0 Accuracy: 0.7581, Class 1 Accuracy: 0.7329\n",
      "F1 Score: 0.3435, Precision: 0.2243, Recall: 0.7329\n",
      "\n",
      "Epoch 100, Loss: 0.7997\n",
      "Class 0 Accuracy: 0.7576, Class 1 Accuracy: 0.7359\n",
      "F1 Score: 0.3442, Precision: 0.2247, Recall: 0.7359\n",
      "\n",
      "Epoch 110, Loss: 0.7935\n",
      "Class 0 Accuracy: 0.7708, Class 1 Accuracy: 0.7351\n",
      "F1 Score: 0.3555, Precision: 0.2344, Recall: 0.7351\n",
      "\n",
      "Epoch 120, Loss: 0.7918\n",
      "Class 0 Accuracy: 0.7485, Class 1 Accuracy: 0.7570\n",
      "F1 Score: 0.3448, Precision: 0.2232, Recall: 0.7570\n",
      "\n",
      "Epoch 130, Loss: 0.7890\n",
      "Class 0 Accuracy: 0.7498, Class 1 Accuracy: 0.7621\n",
      "F1 Score: 0.3478, Precision: 0.2253, Recall: 0.7621\n",
      "\n",
      "Epoch 140, Loss: 0.7857\n",
      "Class 0 Accuracy: 0.7550, Class 1 Accuracy: 0.7620\n",
      "F1 Score: 0.3521, Precision: 0.2289, Recall: 0.7620\n",
      "\n",
      "Epoch 150, Loss: 0.7838\n",
      "Class 0 Accuracy: 0.7601, Class 1 Accuracy: 0.7586\n",
      "F1 Score: 0.3552, Precision: 0.2319, Recall: 0.7586\n",
      "\n",
      "Epoch 160, Loss: 0.7800\n",
      "Class 0 Accuracy: 0.7692, Class 1 Accuracy: 0.7536\n",
      "F1 Score: 0.3613, Precision: 0.2376, Recall: 0.7536\n",
      "\n",
      "Epoch 170, Loss: 0.7770\n",
      "Class 0 Accuracy: 0.7475, Class 1 Accuracy: 0.7821\n",
      "F1 Score: 0.3533, Precision: 0.2282, Recall: 0.7821\n",
      "\n",
      "Epoch 180, Loss: 0.7779\n",
      "Class 0 Accuracy: 0.7533, Class 1 Accuracy: 0.7721\n",
      "F1 Score: 0.3544, Precision: 0.2300, Recall: 0.7721\n",
      "\n",
      "Epoch 190, Loss: 0.7734\n",
      "Class 0 Accuracy: 0.7656, Class 1 Accuracy: 0.7702\n",
      "F1 Score: 0.3645, Precision: 0.2387, Recall: 0.7702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "weighted_model = train_balanced_model(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "58d461ca-ed7b-4ff2-868a-860b8f41c1df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation:\n",
      "Accuracy: 0.7799\n",
      "Precision: 0.2565\n",
      "Recall: 0.8040\n",
      "F1 Score: 0.3889\n",
      "\n",
      "Confusion Matrix:\n",
      "TN: 134221, FP: 38394\n",
      "FN: 3230, TP: 13246\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(weighted_model, data, labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 2.4.0",
   "language": "python",
   "name": "pytorch-2.4.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
